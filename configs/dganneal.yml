training:
  batch_size: 16
  n_epochs: 500000
  n_iters: 200001
  ngpu: 1
  snapshot_freq: 20000
  n_steps_each: 1000
  algo: 'dsm'
  anneal_power: 2.0

data:
  ## mnist
  # dataset: "MNIST"
  # image_size: 28
  # channels: 1
  # logit_transform: false
  # random_flip: false
  
  ## celeba
  # dataset: "CELEBA"
  # image_size: 32
  # channels: 3
  # logit_transform: false
  # random_flip: true

  ## cifar10
  # dataset: "CIFAR10"
  # image_size: 32
  # channels: 3
  # logit_transform: false
  # random_flip: true

  # # celeba-blond
  # dataset: "CELEBA-BLOND"
  # image_size: 32
  # channels: 3
  # logit_transform: false
  # random_flip: true
  # target_domain: 2

  # # VLCS
  # dataset: "VLCS"
  # image_size: 32
  # channels: 3
  # logit_transform: false
  # random_flip: true
  # target_domain: 3

  # # PACS
  # dataset: "PACS"
  # image_size: 32
  # channels: 3
  # logit_transform: false
  # random_flip: true
  # target_domain: 3

  # NICO
  dataset: "NICO"
  image_size: 32
  channels: 3
  logit_transform: false
  random_flip: true
  target_domain: 3

model:
  sigma_begin: 1
  sigma_end: 0.01
  num_classes: 10
  batch_norm: false
  # configurations for CelebA, CIFAR10
  ngf: 128
  # # configurations for MNIST
  # ngf: 64
  # # configurations for DG dataset
  # ngf: 256

optim:
  weight_decay: 0.000
  optimizer: "Adam"
  lr: 0.001
  beta1: 0.9
  amsgrad: false
